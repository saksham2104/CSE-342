{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced952b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dataset \n",
    "I = np.eye(2)\n",
    "\n",
    "label_0 = np.random.multivariate_normal(mean=[-1, -1], cov=I, size=10)\n",
    "label_1 = np.random.multivariate_normal(mean=[1, 1], cov=I, size=10)\n",
    "\n",
    "y0 = np.zeros((10, 1))\n",
    "y1 = np.ones((10, 1))\n",
    "\n",
    "\n",
    "X = np.vstack((label_0, label_1))\n",
    "y = np.vstack((y0, y1))\n",
    "\n",
    "X_train = np.vstack((label_0[:5], label_1[:5]))\n",
    "y_train = np.vstack((y0[:5], y1[:5]))\n",
    "\n",
    "X_test = np.vstack((label_0[5:], label_1[5:]))\n",
    "y_test = np.vstack((y0[5:], y1[5:]))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_dash(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Randomize iniial parametres ->  weights and biases\n",
    "w1 = np.random.randn(2, 1)\n",
    "b1 = np.random.randn(1)\n",
    "\n",
    "w2 = np.random.randn(1, 1)\n",
    "b2 = np.random.randn(1)\n",
    "\n",
    "\n",
    "lr = 0.1 # Learning rate\n",
    "\n",
    "for iteration in range(1000):\n",
    "    total_loss = 0\n",
    "    for x, y_true in zip(X_train, y_train):\n",
    "        x = x.reshape(1,2)       \n",
    "        y_true = y_true.reshape(1, 1) \n",
    "        # Forward pass\n",
    "        z1 = x @ w1 + b1           # (1, 1)\n",
    "        a1 = sigmoid(z1)           # (1, 1)\n",
    "        y_pred = a1 @ w2 + b2      # (1, 1)\n",
    "\n",
    "        #  Loss (MSE)\n",
    "        loss = 0.5 * (y_pred - y_true) ** 2\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #  Backprop and update using gradient descent\n",
    "        dL_dy = y_pred - y_true         # (1, 1)\n",
    "        dL_dw2 = a1.T @ dL_dy           # (1, 1)\n",
    "        dL_db2 = dL_dy.item()\n",
    "\n",
    "        dL_da1 = dL_dy @ w2.T           # (1, 1)\n",
    "        da1_dz1 = sigmoid_dash(z1)      # (1, 1)\n",
    "        dL_dz1 = dL_da1 * da1_dz1       # (1, 1)\n",
    "        dL_dw1 = x.T @ dL_dz1           # (2, 1)\n",
    "        dL_db1 = dL_dz1.item()\n",
    "\n",
    "        #update -> W = W - lr*dl/dw\n",
    "        w2 -= lr * dL_dw2\n",
    "        b2 -= lr * dL_db2\n",
    "        w1 -= lr * dL_dw1\n",
    "        b1 -= lr * dL_db1\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        print(f\"iteration {iteration}: Training Loss = {avg_loss:.4f}\")\n",
    "        print(f\"  w1 = {w1.ravel()}, b1 = {b1.item():.4f}\")\n",
    "        print(f\"  w2 = {w2.ravel()}, b2 = {b2.item():.4f}\\n\")\n",
    "\n",
    "    if iteration == 999:\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        print(f\"iteration {iteration}: Training Loss = {avg_loss:.4f}\")\n",
    "\n",
    "def predict(X):\n",
    "    z1 = X @ w1 + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    y_hat = a1 @ w2 + b2\n",
    "    return y_hat\n",
    "\n",
    "y_pred = predict(X_test)\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(f\"\\nTest MSE: {mse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
