{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f3afb7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from numpy.linalg import svd   # NumPy’s built‑in SVD\n",
    "\n",
    "# ---------- helper functions -------------------------------------\n",
    "def pca_fit(X, k):\n",
    "    \"\"\"\n",
    "    Compute the first k principal components of X (n_samples × n_features)\n",
    "    using thin SVD.\n",
    "\n",
    "    Returns:\n",
    "        mu   : mean row vector, shape (1, d)\n",
    "        V_k  : top‑k right singular vectors, shape (k, d)\n",
    "    \"\"\"\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    Xc = X - mu\n",
    "    # thin SVD: Xc = U Σ Vᵀ\n",
    "    _, _, Vt = svd(Xc, full_matrices=False)\n",
    "    V_k = Vt[:k]           # take top‑k rows\n",
    "    return mu, V_k\n",
    "\n",
    "def pca_transform(X, mu, V_k):\n",
    "    \"\"\"Project new data X onto previously fitted principal axes.\"\"\"\n",
    "    return (X - mu) @ V_k.T\n",
    "\n",
    "#Load dataset and apply PCA reduce to 5D\n",
    "def load_mnist_01(train_per_class=1000, val_frac=0.1):\n",
    "    (X_train_full, y_train_full), (X_test_full, y_test_full) = mnist.load_data()\n",
    "\n",
    "    mask_tr = np.isin(y_train_full, (0, 1))\n",
    "    mask_te = np.isin(y_test_full,  (0, 1))\n",
    "    X_tr_01, y_tr_01 = X_train_full[mask_tr], y_train_full[mask_tr]\n",
    "    X_te_01, y_te_01 = X_test_full[mask_te],  y_test_full[mask_te]\n",
    "\n",
    "    idx0 = np.where(y_tr_01 == 0)[0][:train_per_class]   \n",
    "    idx1 = np.where(y_tr_01 == 1)[0][:train_per_class]   \n",
    "    idx  = np.concatenate([idx0, idx1])                  \n",
    "\n",
    "    X_small, y_small = X_tr_01[idx], y_tr_01[idx]\n",
    "    \n",
    "    n_train = int(len(X_small) * (1 - val_frac)) \n",
    "    X_train, X_val = X_small[:n_train], X_small[n_train:]\n",
    "    y_train, y_val = y_small[:n_train], y_small[n_train:]\n",
    "\n",
    "\n",
    "    X_tr_f = X_train.reshape(len(X_train), -1).astype(np.float32) / 255.0\n",
    "    X_val_f = X_val.reshape(len(X_val), -1).astype(np.float32) / 255.0\n",
    "    X_te_f = X_te_01.reshape(len(X_te_01), -1).astype(np.float32) / 255.0\n",
    "\n",
    "    #PCA\n",
    "    mu, V_k = pca_fit(X_tr_f, k=5)      # fit on TRAIN only\n",
    "    X_tr_p  = pca_transform(X_tr_f,  mu, V_k)\n",
    "    X_val_p = pca_transform(X_val_f, mu, V_k)\n",
    "    X_te_p  = pca_transform(X_te_f,  mu, V_k)\n",
    "\n",
    "\n",
    "    y_tr_b = np.where(y_train == 0, -1, +1)\n",
    "    y_val_b = np.where(y_val == 0, -1, +1)\n",
    "    y_te_b = np.where(y_te_01 == 0, -1, +1)\n",
    "\n",
    "    return (X_tr_p, y_tr_b,\n",
    "            X_val_p, y_val_b,\n",
    "            X_te_p,  y_te_b)\n",
    "\n",
    "\n",
    "def best_stump(X, y, w): #find best stump \n",
    "\n",
    "    n, d = X.shape\n",
    "    best = (None, None, None, 1.0)   \n",
    "\n",
    "    for dim in range(d):\n",
    "        x = X[:, dim]\n",
    "        mn, mx = x.min(), x.max()\n",
    "        steps = np.linspace(mn, mx, 4)[1:-1]   \n",
    "\n",
    "        for thr in steps:\n",
    "            for polarity in (+1, -1):\n",
    "                preds = polarity * np.where(x < thr, +1, -1)\n",
    "                miss  = preds != y\n",
    "                eps   = w[miss].sum()\n",
    "                if eps < best[3]:\n",
    "                    best = (dim, thr, polarity, eps)\n",
    "\n",
    "    return best \n",
    "\n",
    "def adaboost(X, y, X_val, y_val, X_test, y_test,rounds=200):\n",
    "                   \n",
    "    n = len(X)\n",
    "    w = np.full(n, 1 / n)\n",
    "    learners = []        \n",
    "\n",
    "    loss_tr, loss_val, loss_te = [], [], []\n",
    "    err_train = []\n",
    "\n",
    "    def exp_loss(X_, y_, learners_):\n",
    "        F = np.zeros(len(X_))\n",
    "        for dim, thr, pol, b in learners_:\n",
    "            F += b * pol * np.where(X_[:, dim] < thr, +1, -1)\n",
    "        return np.exp(-y_ * F).mean()   \n",
    "\n",
    "    def zero_one_err(X_, y_, learners_):\n",
    "        F = np.zeros(len(X_))\n",
    "        for dim, thr, pol, b in learners_:\n",
    "            F += b * pol * np.where(X_[:, dim] < thr, +1, -1)\n",
    "        return (np.sign(F) != y_).mean()\n",
    "\n",
    "    for m in range(rounds):\n",
    "        dim, thr, pol, eps = best_stump(X, y, w)\n",
    "\n",
    "        eps = np.clip(eps, 1e-12, 1 - 1e-12)\n",
    "        beta = 0.5 * np.log((1 - eps) / eps)\n",
    "\n",
    "        learners.append((dim, thr, pol, beta))\n",
    "\n",
    "        preds = pol * np.where(X[:, dim] < thr, +1, -1)\n",
    "        w *= np.exp(-beta * y * preds)\n",
    "        w /= w.sum()\n",
    "\n",
    "        loss_tr.append(exp_loss(X,y,learners))\n",
    "        loss_val.append(exp_loss(X_val, y_val,learners))\n",
    "        loss_te.append(exp_loss(X_test, y_test,learners))\n",
    "        err_train.append(zero_one_err(X, y,learners))\n",
    "\n",
    "        if m % 40 == 0 : \n",
    "            print(f\"Round {m+1:3d}: eps={eps:.3f}, beta={beta:.3f}, \"\n",
    "                f\"train err={err_train[-1]*100:.20f}%\")\n",
    "\n",
    "    return learners, loss_tr, loss_val, loss_te, err_train\n",
    "\n",
    "def plot(loss_tr, loss_val, loss_te, err_tr):\n",
    "    rounds = np.arange(1, len(loss_tr) + 1)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Plot-1 Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rounds, loss_tr, label='train')\n",
    "    plt.plot(rounds, loss_val, label='val')\n",
    "    plt.plot(rounds, loss_te, label='test')\n",
    "    plt.xlabel(\"Boosting rounds\")\n",
    "    plt.ylabel(\"Exponential loss\")\n",
    "    plt.title(\"Loss vs rounds\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot-2 Training error\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(rounds, err_tr, color='tab:red')\n",
    "    plt.xlabel(\"Boosting rounds\")\n",
    "    plt.ylabel(\"Training error (0‑1)\")\n",
    "    plt.title(\"Training error vs rounds\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_tr, y_tr, X_val, y_val, X_te, y_te = load_mnist_01()\n",
    "\n",
    "    learners, loss_tr, loss_val, loss_te, err_tr = adaboost(\n",
    "        X_tr, y_tr, X_val, y_val, X_te, y_te, rounds=200\n",
    "    )\n",
    "\n",
    "    def predict(X):\n",
    "        F = np.zeros(len(X))\n",
    "        for dim, thr, pol, b in learners:\n",
    "            F += b * pol * np.where(X[:, dim] < thr, +1, -1) #final function which is sum of all weak learners\n",
    "        return np.sign(F)\n",
    "\n",
    "    test_acc = (predict(X_te) == y_te).mean() * 100 \n",
    "    print(f\"\\nFinal test accuracy: {test_acc:.2f}%\") #final test accuracy\n",
    "\n",
    "    plot(loss_tr, loss_val, loss_te, err_tr)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
